{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\tImplementation Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize_scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent (GD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent:\n",
    "    def __init__(self, func, grad, initial_point, learning_rate=0.001):\n",
    "        self.func = func              # Objective function\n",
    "        self.grad = grad              # Gradient function\n",
    "        self.x = np.array(initial_point)  # Initial point\n",
    "        self.lr = learning_rate       # Learning rate\n",
    "        grad_x = self.grad(self.x) #gradient and its norm\n",
    "        grad_norm = np.linalg.norm(grad_x) \n",
    "        # Record \n",
    "        self.history = {\n",
    "            'x': [self.x.copy()], \n",
    "            'f': [func(self.x)],\n",
    "            'grad_norm': [grad_norm]\n",
    "        }\n",
    "    \n",
    "    def set_learning_rate(self, new_lr):\n",
    "        self.lr = new_lr\n",
    "\n",
    "    def step(self):\n",
    "        grad_x = self.grad(self.x)      # Compute current gradient\n",
    "        self.x -= self.lr * grad_x      # Update rule:\n",
    "        self.history['x'].append(self.x.copy())         # Record new point\n",
    "        self.history['f'].append(self.func(self.x))     # Record function value\n",
    "        self.history['grad_norm'].append(np.linalg.norm(self.grad(self.x)))  # Record gradient norm\n",
    "\n",
    "    def run(self, max_iter=1000, tol=1e-6):\n",
    "        for _ in range(max_iter):\n",
    "            if np.linalg.norm(self.grad(self.x)) < tol:\n",
    "                break\n",
    "            self.step()\n",
    "        return self.x, self.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjugate Gradient (CG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConjugateGradient:\n",
    "    def __init__(self, func, grad, initial_point, initial_step_size=1.0, decay_factor=1.0):\n",
    "        self.func = func              \n",
    "        self.grad = grad              \n",
    "        self.x = np.array(initial_point)  \n",
    "        self.grad_x = self.grad(self.x) \n",
    "        self.d = -self.grad_x   # Initial search direction\n",
    "        self.initial_step = initial_step_size\n",
    "        self.decay_factor = decay_factor\n",
    "        self.iter_count = 0\n",
    "        self.history = {\n",
    "            'x': [self.x.copy()], \n",
    "            'f': [func(self.x)], \n",
    "            'grad_norm': [np.linalg.norm(self.grad_x)]\n",
    "        }\n",
    "\n",
    "    def exact_line_search(self, x, d):\n",
    "        #g(alpha) = f(x + alpha*d)\n",
    "        def g(alpha):\n",
    "            return self.func(x + alpha * d)\n",
    "        result = minimize_scalar(g)\n",
    "        \n",
    "        # Using decay factor to adjust step size\n",
    "        if self.decay_factor < 1.0:\n",
    "            decayed_step = self.initial_step * (self.decay_factor ** self.iter_count)\n",
    "            return min(result.x, decayed_step)\n",
    "        return result.x  \n",
    "\n",
    "    def step(self):\n",
    "        self.iter_count += 1\n",
    "        alpha = self.exact_line_search(self.x, self.d)\n",
    "        self.x += alpha * self.d\n",
    "        grad_new = self.grad(self.x)\n",
    "        \n",
    "        # Fletcher-Reeves formula\n",
    "        beta = np.dot(grad_new, grad_new) / np.dot(self.grad_x, self.grad_x)\n",
    "        self.d = -grad_new + beta * self.d\n",
    "        self.grad_x = grad_new\n",
    "        self.history['x'].append(self.x.copy())\n",
    "        self.history['f'].append(self.func(self.x))\n",
    "        self.history['grad_norm'].append(np.linalg.norm(grad_new))\n",
    "        \n",
    "    def run(self, max_iter=1000, tol=1e-6):\n",
    "        for i in range(max_iter):\n",
    "            # termination condition\n",
    "            if np.linalg.norm(self.grad_x) < tol:\n",
    "                break\n",
    "            self.step()\n",
    "            if (i+1) % len(self.x) == 0:\n",
    "                self.d = -self.grad_x\n",
    "        return self.x, self.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteepestDescent:\n",
    "    def __init__(self, func, grad, initial_point, initial_step_size=1.0, decay_factor=1.0):\n",
    "        self.func = func              \n",
    "        self.grad = grad              \n",
    "        self.x = np.array(initial_point)  \n",
    "        self.initial_step = initial_step_size  \n",
    "        self.decay_factor = decay_factor  # decay factor\n",
    "        self.iter_count = 0          \n",
    "        self.history = {\n",
    "            'x': [self.x.copy()], \n",
    "            'f': [func(self.x)], \n",
    "            'grad_norm': [np.linalg.norm(grad(self.x))]\n",
    "        }\n",
    "\n",
    "    def exact_line_search(self, x, d):\n",
    "    # define one-dimensional function g(alpha) = f(x + alpha*d)\n",
    "        def g(alpha):\n",
    "            return self.func(x + alpha * d)\n",
    "    \n",
    "        # found minimum of g(alpha)\n",
    "        result = minimize_scalar(g)\n",
    "        \n",
    "        # apply step size decay\n",
    "        if self.decay_factor < 1.0:\n",
    "            decayed_step = self.initial_step * (self.decay_factor ** (self.iter_count - 1))\n",
    "            return min(result.x, decayed_step)\n",
    "        \n",
    "        return result.x\n",
    "\n",
    "    def step(self):\n",
    "        self.iter_count += 1\n",
    "        \n",
    "        d = -self.grad(self.x)\n",
    "        alpha = self.exact_line_search(self.x, d)\n",
    "        # update\n",
    "        self.x += alpha * d\n",
    "        self.history['x'].append(self.x.copy())\n",
    "        self.history['f'].append(self.func(self.x))\n",
    "        self.history['grad_norm'].append(np.linalg.norm(self.grad(self.x)))\n",
    "    \n",
    "    def run(self, max_iter=1000, tol=1e-6):\n",
    "        for i in range(max_iter):\n",
    "            if np.linalg.norm(self.grad(self.x)) < tol:\n",
    "                break\n",
    "            self.step()\n",
    "            \n",
    "        return self.x, self.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Gradient Descent (MGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumGD:\n",
    "    def __init__(self, func, grad, initial_point, learning_rate=0.001, momentum=0.9):\n",
    "        self.func = func              \n",
    "        self.grad = grad              \n",
    "        self.x = np.array(initial_point) \n",
    "        self.lr = learning_rate       \n",
    "        self.mu = momentum            # momentum coefficient \n",
    "        self.v = np.zeros_like(self.x)  # Initialize velocity\n",
    "        self.iter_count = 0\n",
    "        grad_x = self.grad(self.x)\n",
    "        self.history = {\n",
    "            'x': [self.x.copy()], \n",
    "            'f': [func(self.x)],\n",
    "            'grad_norm': [np.linalg.norm(grad_x)]\n",
    "        }\n",
    "\n",
    "    def step(self):\n",
    "        self.iter_count += 1\n",
    "        grad_x = self.grad(self.x)\n",
    "        self.v = self.mu * self.v - self.lr * grad_x  # update velocity\n",
    "        self.x += self.v             \n",
    "        self.history['x'].append(self.x.copy())  \n",
    "        self.history['f'].append(self.func(self.x))\n",
    "        self.history['grad_norm'].append(np.linalg.norm(self.grad(self.x)))\n",
    "\n",
    "    def run(self, max_iter=1000, tol=1e-6):\n",
    "        for _ in range(max_iter):\n",
    "            if np.linalg.norm(self.grad(self.x)) < tol:\n",
    "                break\n",
    "                \n",
    "            self.step()\n",
    "            \n",
    "        return self.x, self.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewtonMethod:\n",
    "    def __init__(self, func, grad, hessian, initial_point):\n",
    "        self.func = func             \n",
    "        self.grad = grad              \n",
    "        self.hessian = hessian        # Hessian matrix\n",
    "        self.x = np.array(initial_point)\n",
    "        grad_x = self.grad(self.x)\n",
    "        grad_norm = np.linalg.norm(grad_x)\n",
    "        self.history = {\n",
    "            'x': [self.x.copy()], \n",
    "            'f': [func(self.x)],\n",
    "            'grad_norm': [grad_norm]\n",
    "        }\n",
    "\n",
    "    def step(self):\n",
    "        g = self.grad(self.x)         # current gradient g^(k) = âˆ‡f(x^(k))\n",
    "        H = self.hessian(self.x)      # current Hessian\n",
    "        \n",
    "        # Check if Hessian is positive definite\n",
    "        try:\n",
    "            p = np.linalg.solve(H, -g)\n",
    "        except np.linalg.LinAlgError:\n",
    "            p = -np.linalg.pinv(H) @ g\n",
    "            \n",
    "        # update rule\n",
    "        self.x += p\n",
    "        \n",
    "        new_grad = self.grad(self.x)\n",
    "        grad_norm = np.linalg.norm(new_grad)\n",
    "        self.history['x'].append(self.x.copy())\n",
    "        self.history['f'].append(self.func(self.x))\n",
    "        self.history['grad_norm'].append(grad_norm)\n",
    "    \n",
    "    def run(self, max_iter=100, tol=1e-6):\n",
    "        for _ in range(max_iter):\n",
    "            if self.history['grad_norm'][-1] < tol:\n",
    "                break\n",
    "            self.step()\n",
    "        return self.x, self.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quasi-Newton Method - BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BFGS:\n",
    "    def __init__(self, func, grad, initial_point, initial_step_size=1.0, decay_factor=1.0):\n",
    "        self.func = func              \n",
    "        self.grad = grad              \n",
    "        self.x = np.array(initial_point)  \n",
    "        self.Q = np.eye(len(initial_point))  # Initial inverse Hessian approximation\n",
    "        self.initial_step = initial_step_size\n",
    "        self.decay_factor = decay_factor\n",
    "        self.iter_count = 0\n",
    "        grad_x = self.grad(self.x)\n",
    "        grad_norm = np.linalg.norm(grad_x)\n",
    "        self.history = {\n",
    "            'x': [self.x.copy()], \n",
    "            'f': [func(self.x)],\n",
    "            'grad_norm': [grad_norm]\n",
    "        }\n",
    "\n",
    "    def exact_line_search(self, x, d):\n",
    "        def g(alpha):\n",
    "            return self.func(x + alpha * d)\n",
    "        result = minimize_scalar(g)\n",
    "\n",
    "        if self.decay_factor < 1.0:\n",
    "            decayed_step = self.initial_step * (self.decay_factor ** self.iter_count)\n",
    "            return min(result.x, decayed_step)\n",
    "        return result.x\n",
    "\n",
    "    def step(self):\n",
    "        self.iter_count += 1\n",
    "        g = self.grad(self.x)         # Current gradient\n",
    "        d = -self.Q @ g               # Search direction\n",
    "        alpha = self.exact_line_search(self.x, d)  # Line search\n",
    "        \n",
    "        # Update position\n",
    "        delta = alpha * d             \n",
    "        self.x += delta               \n",
    "        # Calculate gradient change\n",
    "        g_new = self.grad(self.x)\n",
    "        gamma = g_new - g             \n",
    "        \n",
    "        # inverse Hessian approximation\n",
    "        if np.dot(delta, gamma) > 1e-10:  # Ensure curvature condition\n",
    "            # BFGS update formula\n",
    "            term1 = (np.outer(delta, gamma) @ self.Q + self.Q @ np.outer(gamma, delta)) / np.dot(delta, gamma)\n",
    "            term2 = (1 + np.dot(gamma, self.Q @ gamma) / np.dot(delta, gamma)) * np.outer(delta, delta) / np.dot(delta, gamma)\n",
    "            self.Q = self.Q - term1 + term2\n",
    "        self.history['x'].append(self.x.copy())\n",
    "        self.history['f'].append(self.func(self.x))\n",
    "        self.history['grad_norm'].append(np.linalg.norm(g_new))\n",
    "    \n",
    "    def run(self, max_iter=100, tol=1e-6):\n",
    "        for _ in range(max_iter):\n",
    "            if self.history['grad_norm'][-1] < tol:\n",
    "                break\n",
    "                \n",
    "            self.step()\n",
    "            \n",
    "        return self.x, self.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowellMethod:\n",
    "    def __init__(self, func, initial_point, initial_step_size=1.0, decay_factor=1.0):\n",
    "        self.func = func           \n",
    "        self.x = np.array(initial_point)  \n",
    "        self.directions = np.eye(len(initial_point))  # Initial direction set\n",
    "        self.initial_step = initial_step_size\n",
    "        self.decay_factor = decay_factor\n",
    "        self.iter_count = 0\n",
    "        self.history = {\n",
    "            'x': [self.x.copy()], \n",
    "            'f': [func(self.x)]\n",
    "        }\n",
    "\n",
    "    def exact_line_search(self, x, d):\n",
    "        def g(alpha):\n",
    "            return self.func(x + alpha * d)\n",
    "        result = minimize_scalar(g)\n",
    "\n",
    "        if self.decay_factor < 1.0:\n",
    "            decayed_step = self.initial_step * (self.decay_factor ** self.iter_count)\n",
    "            return min(result.x, decayed_step)\n",
    "        return result.x\n",
    "\n",
    "    def step(self):\n",
    "        self.iter_count += 1\n",
    "        x0 = self.x.copy()            \n",
    "        \n",
    "        # line minimization\n",
    "        for i, d in enumerate(self.directions):\n",
    "            alpha = self.exact_line_search(self.x, d)\n",
    "            self.x += alpha * d\n",
    "        \n",
    "        # Fresh conjugate direction\n",
    "        d_new = self.x - x0\n",
    "        \n",
    "        # Normalize <- non-zero\n",
    "        norm = np.linalg.norm(d_new)\n",
    "        if norm > 1e-10:\n",
    "            d_new_normalized = d_new / norm\n",
    "            \n",
    "            # line minimization\n",
    "            alpha = self.exact_line_search(self.x, d_new)\n",
    "            self.x += alpha * d_new\n",
    "            \n",
    "            # Update the set of directions\n",
    "            self.directions = np.roll(self.directions, -1, axis=0)\n",
    "            self.directions[-1] = d_new_normalized\n",
    "        \n",
    "        self.history['x'].append(self.x.copy())\n",
    "        self.history['f'].append(self.func(self.x))\n",
    "    \n",
    "    def run(self, max_iter=100, tol=1e-6):\n",
    "        for i in range(max_iter):\n",
    "            x_old = self.x.copy()\n",
    "            self.step()\n",
    "            # Check convergence\n",
    "            if np.linalg.norm(self.x - x_old) < tol:\n",
    "                break\n",
    "                \n",
    "        return self.x, self.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize_scalar\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rosenbrock Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x):\n",
    "    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n",
    "\n",
    "#Gradient\n",
    "def rosenbrock_grad(x):\n",
    "    dx = -2*(1 - x[0]) - 400*x[0]*(x[1] - x[0]**2)\n",
    "    dy = 200*(x[1] - x[0]**2)\n",
    "    return np.array([dx, dy])\n",
    "\n",
    "#Hessian\n",
    "def rosenbrock_hess(x):\n",
    "    H = np.zeros((2, 2))\n",
    "    H[0, 0] = 2 - 400*x[1] + 1200*x[0]**2\n",
    "    H[0, 1] = -400*x[0]\n",
    "    H[1, 0] = -400*x[0]\n",
    "    H[1, 1] = 200\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Himmelblau Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def himmelblau(x):\n",
    "    return (x[0]**2 + x[1] - 11)**2 + (x[0] + x[1]**2 - 7)**2\n",
    "\n",
    "#Gradient\n",
    "def himmelblau_grad(x):\n",
    "    dx = 4*x[0]*(x[0]**2 + x[1] - 11) + 2*(x[0] + x[1]**2 - 7)\n",
    "    dy = 2*(x[0]**2 + x[1] - 11) + 4*x[1]*(x[0] + x[1]**2 - 7)\n",
    "    return np.array([dx, dy])\n",
    "\n",
    "#Hessian\n",
    "def himmelblau_hess(x):\n",
    "    \"\"\"Hessian of Himmelblau function\"\"\"\n",
    "    H = np.zeros((2, 2))\n",
    "    H[0, 0] = 12*x[0]**2 + 4*x[1] - 42\n",
    "    H[0, 1] = 4*x[0] + 4*x[1]\n",
    "    H[1, 0] = 4*x[0] + 4*x[1]\n",
    "    H[1, 1] = 2 + 12*x[1]**2 + 4*x[0] - 14\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High-Dimensional Quadratic Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_quadratic_function(n=5, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Create positive definite matrix Q\n",
    "    A = np.random.randn(n, n)\n",
    "    Q = A.T @ A + 0.1 * np.eye(n)\n",
    "    b = np.random.randn(n)\n",
    "    \n",
    "    def quadratic(x):\n",
    "        return 0.5 * x @ Q @ x - b @ x\n",
    "    \n",
    "    def quadratic_grad(x):\n",
    "        return Q @ x - b\n",
    "    \n",
    "    def quadratic_hess(x):\n",
    "        return Q\n",
    "    \n",
    "    # True minimum\n",
    "    true_min = np.linalg.solve(Q, b)\n",
    "    true_min_value = quadratic(true_min)\n",
    "    \n",
    "    return quadratic, quadratic_grad, quadratic_hess, Q, b, true_min, true_min_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ackley Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ackley(x):\n",
    "    term1 = -20 * np.exp(-0.2 * np.sqrt(0.5 * (x[0]**2 + x[1]**2)))\n",
    "    term2 = -np.exp(0.5 * (np.cos(2 * np.pi * x[0]) + np.cos(2 * np.pi * x[1])))\n",
    "    return term1 + term2 + np.e + 20\n",
    "\n",
    "# Gradient\n",
    "def ackley_grad(x):\n",
    "    sqrt_term = np.sqrt(0.5 * (x[0]**2 + x[1]**2))\n",
    "    exp1 = np.exp(-0.2 * sqrt_term)\n",
    "    exp2 = np.exp(0.5 * (np.cos(2 * np.pi * x[0]) + np.cos(2 * np.pi * x[1])))\n",
    "    \n",
    "    # first term of the gradient\n",
    "    if sqrt_term < 1e-10:  \n",
    "        common1 = 0\n",
    "    else:\n",
    "        common1 = 20 * 0.2 * 0.5 * exp1 / sqrt_term\n",
    "    \n",
    "    dx1 = common1 * x[0]\n",
    "    dy1 = common1 * x[1]\n",
    "    \n",
    "    # Second term of the gradient\n",
    "    dx2 = np.pi * exp2 * np.sin(2 * np.pi * x[0])\n",
    "    dy2 = np.pi * exp2 * np.sin(2 * np.pi * x[1])\n",
    "    \n",
    "    return np.array([dx1 - dx2, dy1 - dy2])\n",
    "\n",
    "\n",
    "# Hessian\n",
    "def ackley_hess(x):\n",
    "    eps = 1e-6  \n",
    "    hess = np.zeros((2, 2))\n",
    "    \n",
    "    grad_x = ackley_grad(x)\n",
    "    \n",
    "    # compute first column of Hessian\n",
    "    x_plus = x.copy()\n",
    "    x_plus[0] += eps\n",
    "    grad_x_plus = ackley_grad(x_plus)\n",
    "    hess[:, 0] = (grad_x_plus - grad_x) / eps\n",
    "    \n",
    "    # compute second column of Hessian\n",
    "    x_plus = x.copy()\n",
    "    x_plus[1] += eps\n",
    "    grad_x_plus = ackley_grad(x_plus)\n",
    "    hess[:, 1] = (grad_x_plus - grad_x) / eps\n",
    "    \n",
    "    return hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beale Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beale(x):\n",
    "    term1 = (1.5 - x[0] + x[0]*x[1])**2\n",
    "    term2 = (2.25 - x[0] + x[0]*(x[1]**2))**2\n",
    "    term3 = (2.625 - x[0] + x[0]*(x[1]**3))**2\n",
    "    return term1 + term2 + term3\n",
    "\n",
    "# Gradient\n",
    "def beale_grad(x):\n",
    "    term1 = 1.5 - x[0] + x[0]*x[1]\n",
    "    term2 = 2.25 - x[0] + x[0]*(x[1]**2)\n",
    "    term3 = 2.625 - x[0] + x[0]*(x[1]**3)\n",
    "    \n",
    "    dx = 2*term1*(-1 + x[1]) + \\\n",
    "         2*term2*(-1 + x[1]**2) + \\\n",
    "         2*term3*(-1 + x[1]**3)\n",
    "    \n",
    "    dy = 2*term1*(x[0]) + \\\n",
    "         2*term2*(2*x[0]*x[1]) + \\\n",
    "         2*term3*(3*x[0]*(x[1]**2))\n",
    "    \n",
    "    return np.array([dx, dy])\n",
    "\n",
    "# Hessian\n",
    "def beale_hess(x):\n",
    "    eps = 1e-6 \n",
    "    hess = np.zeros((2, 2))\n",
    "    \n",
    "    grad_x = beale_grad(x)\n",
    "    \n",
    "    x_plus = x.copy()\n",
    "    x_plus[0] += eps\n",
    "    grad_x_plus = beale_grad(x_plus)\n",
    "    hess[:, 0] = (grad_x_plus - grad_x) / eps\n",
    "    \n",
    "    x_plus = x.copy()\n",
    "    x_plus[1] += eps\n",
    "    grad_x_plus = beale_grad(x_plus)\n",
    "    hess[:, 1] = (grad_x_plus - grad_x) / eps\n",
    "    \n",
    "    return hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sphere Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sphere(x):\n",
    "    return np.sum(x**2)\n",
    "\n",
    "# Gradient\n",
    "def sphere_grad(x):\n",
    "    return 2*x\n",
    "\n",
    "# Hessian\n",
    "def sphere_hess(x):\n",
    "\n",
    "    n = len(x)  \n",
    "    return 2 * np.eye(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Function Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a quadratic function\n",
    "quad_func, quad_grad, quad_hess, Q, b, quad_true_min, quad_true_min_val = create_quadratic_function(5)\n",
    "\n",
    "# Integrate functions\n",
    "test_functions = [\n",
    "    {\n",
    "        'name': 'Rosenbrock',\n",
    "        'func': rosenbrock,\n",
    "        'grad': rosenbrock_grad,\n",
    "        'hess': rosenbrock_hess,\n",
    "        'initial_point': np.array([-1.2, 1.0]),\n",
    "        'true_minimum': np.array([1.0, 1.0]),\n",
    "        'true_min_value': 0.0\n",
    "    },\n",
    "    {\n",
    "        'name': 'Himmelblau',\n",
    "        'func': himmelblau,\n",
    "        'grad': himmelblau_grad,\n",
    "        'hess': himmelblau_hess,\n",
    "        'initial_point': np.array([-2.0, 2.0]),\n",
    "        'true_minimum': np.array([3.0, 2.0]), \n",
    "        'true_min_value': 0.0\n",
    "    },\n",
    "    {\n",
    "        'name': 'Quadratic',\n",
    "        'func': quad_func,\n",
    "        'grad': quad_grad,\n",
    "        'hess': quad_hess,\n",
    "        'initial_point': np.ones(5) * 2,\n",
    "        'true_minimum': quad_true_min,\n",
    "        'true_min_value': quad_true_min_val\n",
    "    },\n",
    "    {\n",
    "        'name': 'Ackley',\n",
    "        'func': ackley,\n",
    "        'grad': ackley_grad,\n",
    "        'hess': ackley_hess,\n",
    "        'initial_point': np.array([1.5, 1.5]),\n",
    "        'true_minimum': np.array([0.0, 0.0]),\n",
    "        'true_min_value': 0.0\n",
    "    },\n",
    "    {\n",
    "        'name': 'Beale',\n",
    "        'func': beale,\n",
    "        'grad': beale_grad,\n",
    "        'hess': beale_hess,\n",
    "        'initial_point': np.array([1.0, 1.0]),\n",
    "        'true_minimum': np.array([3.0, 0.5]),\n",
    "        'true_min_value': 0.0\n",
    "    },\n",
    "    {\n",
    "        'name': 'Sphere',\n",
    "        'func': sphere,\n",
    "        'grad': sphere_grad,\n",
    "        'hess': sphere_hess,\n",
    "        'initial_point': np.array([3.0, -4.0]),\n",
    "        'true_minimum': np.array([0.0, 0.0]),\n",
    "        'true_min_value': 0.0\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Study - Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimization_tests(algorithms, test_functions):\n",
    "    results = []\n",
    "    \n",
    "    for func_data in test_functions:\n",
    "        func_name = func_data['name']\n",
    "        print(f\"\\n{'-'*20} Test Function: {func_name} {'-'*20}\")\n",
    "        \n",
    "        func_results = []\n",
    "        \n",
    "       # All algorithms\n",
    "        for algo in algorithms:\n",
    "            algo_name = algo['name']\n",
    "            algo_class = algo['class']\n",
    "            \n",
    "            print(f\"\\nRun {algo_name}...\")\n",
    "            \n",
    "            # setup function and parameters\n",
    "            kwargs = {'func': func_data['func'], 'initial_point': func_data['initial_point']}\n",
    "            \n",
    "            if algo.get('needs_grad', False):\n",
    "                kwargs['grad'] = func_data['grad']\n",
    "            \n",
    "            if algo.get('needs_hess', False):\n",
    "                kwargs['hessian'] = func_data['hess']\n",
    "                \n",
    "            # Extra parameters\n",
    "            for k, v in algo.get('params', {}).items():\n",
    "                kwargs[k] = v\n",
    "            \n",
    "            # create optimizer instance\n",
    "            try:\n",
    "                optimizer = algo_class(**kwargs)\n",
    "                \n",
    "                start_time = time.time()\n",
    "                x_opt, history = optimizer.run(max_iter=1000, tol=1e-6)\n",
    "                end_time = time.time()\n",
    "                \n",
    "                # Error\n",
    "                error = np.linalg.norm(x_opt - func_data['true_minimum'])\n",
    "                func_error = abs(func_data['func'](x_opt) - func_data['true_min_value'])\n",
    "                \n",
    "                # Exclude the last iteration\n",
    "                iteration_count = len(history['f']) - 1  \n",
    "                \n",
    "                result = {\n",
    "                    'algorithm': algo_name,\n",
    "                    'iterations': iteration_count,\n",
    "                    'time': end_time - start_time,\n",
    "                    'error': error,\n",
    "                    'func_error': func_error,\n",
    "                    'x_final': x_opt,\n",
    "                    'f_final': history['f'][-1],\n",
    "                    'history': history\n",
    "                }\n",
    "                \n",
    "                func_results.append(result)\n",
    "                \n",
    "                print(f\"{algo_name} completed in {iteration_count} iterations ({end_time - start_time:.4f} seconds)\")\n",
    "                print(f\"  Final x = {x_opt}\")\n",
    "                print(f\"  Final f(x) = {history['f'][-1]:.8e}\")\n",
    "                print(f\"  Error: ||x - x*|| = {error:.8e}, |f(x) - f(x*)| = {func_error:.8e}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"{algo_name} Fail: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        results.append({\n",
    "            'function': func_name,\n",
    "            'results': func_results\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_optimization_results(results):\n",
    "    # Set up for visualization\n",
    "    import os\n",
    "    if not os.path.exists('optimization_results'):\n",
    "        os.makedirs('optimization_results')\n",
    "    import matplotlib.cm as cm\n",
    "    unique_algorithms = set()\n",
    "    for func_result in results:\n",
    "        for algo_result in func_result['results']:\n",
    "            unique_algorithms.add(algo_result['algorithm'])\n",
    "    unique_algorithms = sorted(list(unique_algorithms))\n",
    "    colors = cm.tab10(np.linspace(0, 1, len(unique_algorithms)))\n",
    "    algorithm_colors = {algo: colors[i] for i, algo in enumerate(unique_algorithms)}\n",
    "    markers = ['o', 's', '^', 'd', 'x', '*', '+', 'v', '<', '>']\n",
    "    algorithm_markers = {algo: markers[i % len(markers)] for i, algo in enumerate(unique_algorithms)}\n",
    "    \n",
    "\n",
    "    # 1. Convergency\n",
    "    for func_result in results:\n",
    "        func_name = func_result['function']\n",
    "        print(f\"Creating convergence plots for {func_name}...\")\n",
    "        \n",
    "        # 1.1 Function Value Convergence Plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        for algo_result in func_result['results']:\n",
    "            algo_name = algo_result['algorithm']\n",
    "            history = algo_result['history']\n",
    "            iterations = range(len(history['f']))\n",
    "            plt.semilogy(iterations, history['f'], \n",
    "                       label=algo_name, \n",
    "                       color=algorithm_colors[algo_name],\n",
    "                       marker=algorithm_markers[algo_name], \n",
    "                       markevery=max(1, len(iterations)//10))\n",
    "        \n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Function Value (log scale)')\n",
    "        plt.title(f'Function Value Convergence - {func_name}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'optimization_results/{func_name}_value_convergence.png', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # 1.2 Gradient Norm Convergence Plot (if available)\n",
    "        has_grad_data = False\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        for algo_result in func_result['results']:\n",
    "            algo_name = algo_result['algorithm']\n",
    "            history = algo_result['history']\n",
    "            \n",
    "            # grad norm data to judge\n",
    "            if 'grad_norm' in history and len(history.get('grad_norm', [])) > 0:\n",
    "                has_grad_data = True\n",
    "                iterations = range(len(history['grad_norm']))\n",
    "                plt.semilogy(iterations, history['grad_norm'], \n",
    "                           label=algo_name, \n",
    "                           color=algorithm_colors[algo_name],\n",
    "                           marker=algorithm_markers[algo_name], \n",
    "                           markevery=max(1, len(iterations)//10))\n",
    "        \n",
    "        if has_grad_data:\n",
    "            plt.xlabel('Iterations')\n",
    "            plt.ylabel('Gradient Norm (log scale)')\n",
    "            plt.title(f'Gradient Norm Convergence - {func_name}')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'optimization_results/{func_name}_grad_convergence.png', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    # 2. Performance COMPARISON - Bar charts\n",
    "    metrics = ['iterations', 'time', 'error', 'func_error']\n",
    "    metric_labels = ['Iterations', 'Execution Time (s)', 'Parameter Error', 'Function Value Error']\n",
    "    \n",
    "    # 2.1 Create bar charts for each metric across all functions\n",
    "    for metric, metric_label in zip(metrics, metric_labels):\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        # Group data by function\n",
    "        functions = []\n",
    "        data = []\n",
    "        \n",
    "        for func_result in results:\n",
    "            functions.append(func_result['function'])\n",
    "            func_data = {}\n",
    "            \n",
    "            for algo_result in func_result['results']:\n",
    "                algo_name = algo_result['algorithm']\n",
    "                func_data[algo_name] = algo_result[metric]\n",
    "            \n",
    "            data.append(func_data)\n",
    "        \n",
    "        # Set up bar positions\n",
    "        x = np.arange(len(functions))\n",
    "        width = 0.8 / len(unique_algorithms)\n",
    "        \n",
    "        # Create bars\n",
    "        for i, algo_name in enumerate(unique_algorithms):\n",
    "            values = []\n",
    "            for func_data in data:\n",
    "                values.append(func_data.get(algo_name, 0))\n",
    "            \n",
    "            offset = i * width - 0.4 + width / 2\n",
    "            plt.bar(x + offset, values, width, label=algo_name, color=algorithm_colors[algo_name])\n",
    "        \n",
    "        plt.xlabel('Test Function')\n",
    "        plt.ylabel(metric_label)\n",
    "        plt.title(f'Performance Comparison - {metric_label}')\n",
    "        plt.xticks(x, functions, rotation=45)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Use log scale for error metrics\n",
    "        if 'error' in metric:\n",
    "            plt.yscale('log')\n",
    "        \n",
    "        plt.grid(True, axis='y')\n",
    "        plt.savefig(f'optimization_results/comparison_{metric}.png', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    print(\"\\nVisualization completed. Check the 'optimization_results' directory for all figures.\")\n",
    "    \n",
    "    return \"Visualization completed successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In-depth Study: Control Variates Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Impact of initial point on algorithms performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_initial_point_experiments(algorithm, test_functions):\n",
    "    results = []\n",
    "    \n",
    "   \n",
    "    initial_points = {\n",
    "        'Rosenbrock': [\n",
    "            np.array([-1.2, 1.0]),  \n",
    "            np.array([0.0, 0.0]),   \n",
    "            np.array([2.0, 2.0])   \n",
    "        ],\n",
    "        'Himmelblau': [\n",
    "            np.array([-2.0, 2.0]),  \n",
    "            np.array([1.0, 1.0]),   \n",
    "            np.array([4.0, 4.0])    \n",
    "        ]\n",
    "    }\n",
    "    \n",
    "   \n",
    "    selected_functions = ['Rosenbrock', 'Himmelblau'] \n",
    "    \n",
    "    \n",
    "    \n",
    "    for func_name, points in initial_points.items():\n",
    "        if func_name not in selected_functions:\n",
    "            continue  \n",
    "            \n",
    "      \n",
    "        func_data = next((f for f in test_functions if f['name'] == func_name), None)\n",
    "        if func_data is None:\n",
    "            print(f\"warn: can not find {func_name}\")\n",
    "            continue\n",
    "            \n",
    "        point_results = []\n",
    "        \n",
    "        print(f\"\\nTest {func_name} function's sensitivity on initial point:\")\n",
    "        \n",
    "        for point in points:\n",
    "            print(f\"  Initial Point: {point}\")\n",
    "            \n",
    "            algo_params = {'func': func_data['func']}\n",
    "            if algorithm.get('needs_grad', False):\n",
    "                algo_params['grad'] = func_data['grad']\n",
    "            if algorithm.get('needs_hess', False):\n",
    "                algo_params['hessian'] = func_data['hess']\n",
    "                \n",
    "            \n",
    "            algo_params['initial_point'] = point\n",
    "            \n",
    "          \n",
    "            for k, v in algorithm.get('params', {}).items():\n",
    "                algo_params[k] = v\n",
    "            \n",
    "            optimizer = algorithm['class'](**algo_params)\n",
    "            x_opt, history = optimizer.run(max_iter=1000, tol=1e-6)\n",
    "            \n",
    "          \n",
    "            error = np.linalg.norm(x_opt - func_data['true_minimum'])\n",
    "            func_error = abs(func_data['func'](x_opt) - func_data['true_min_value'])\n",
    "            \n",
    "            point_results.append({\n",
    "                'initial_point': point,\n",
    "                'final_point': x_opt,\n",
    "                'iterations': len(history['f']) - 1,\n",
    "                'error': error,\n",
    "                'func_error': func_error,\n",
    "                'history': history\n",
    "            })\n",
    "            \n",
    "           \n",
    "            print(f\"    Finished in {len(history['f']) - 1} times iterations\")\n",
    "            print(f\"    Final Position: {x_opt}\")\n",
    "            print(f\"    Error: {error:.8e}\")\n",
    "        \n",
    "        results.append({\n",
    "            'function': func_name,\n",
    "            'results': point_results\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_algorithm_comparison(all_results):\n",
    "    save_dir = 'optimization_comparison'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "   \n",
    "    test_functions = set()\n",
    "    for algo_name, results in all_results.items():\n",
    "        for func_result in results:\n",
    "            test_functions.add(func_result['function'])\n",
    "    test_functions = sorted(list(test_functions))\n",
    "    algorithms = list(all_results.keys())\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(algorithms)))\n",
    "    algo_colors = {algo: colors[i] for i, algo in enumerate(algorithms)}\n",
    "    \n",
    "    \n",
    "    for func_name in test_functions:\n",
    "        print(f\"Creating comparison for {func_name} function...\")\n",
    "        \n",
    "        \n",
    "        fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        \n",
    "        # 1. Convergence Speed Comparison\n",
    "        for algo_name, results in all_results.items():\n",
    "            func_result = next((r for r in results if r['function'] == func_name), None)\n",
    "            if not func_result or not func_result['results']:\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            point_result = func_result['results'][0]\n",
    "            history = point_result['history']\n",
    "            iterations = range(len(history['f']))\n",
    "            \n",
    "            axs[0].semilogy(iterations, history['f'], \n",
    "                          label=f\"{algo_name}\",\n",
    "                          color=algo_colors[algo_name],\n",
    "                          marker='o', \n",
    "                          markevery=max(1, len(iterations)//10))\n",
    "        \n",
    "        \n",
    "        axs[0].set_title('Convergence Speed', fontsize=12)\n",
    "        axs[0].set_xlabel('Iterations')\n",
    "        axs[0].set_ylabel('Function Value (log scale)')\n",
    "        axs[0].grid(True)\n",
    "        axs[0].legend(loc='best')\n",
    "        \n",
    "        # 2 & 3. Collect iteration counts and error data for all algorithms\n",
    "        algo_names = []\n",
    "        avg_iterations = []\n",
    "        avg_errors = []\n",
    "        \n",
    "        for algo_name, results in all_results.items():\n",
    "            func_result = next((r for r in results if r['function'] == func_name), None)\n",
    "            if not func_result or not func_result['results']:\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            algo_iterations = [r['iterations'] for r in func_result['results']]\n",
    "            algo_errors = [r['func_error'] for r in func_result['results']]\n",
    "            \n",
    "            algo_names.append(algo_name)\n",
    "            avg_iterations.append(np.mean(algo_iterations))\n",
    "            avg_errors.append(np.mean(algo_errors))\n",
    "        \n",
    "        # 2. Computational Efficiency Comparison (middle panel)\n",
    "        bars = axs[1].bar(algo_names, avg_iterations, \n",
    "                        color=[algo_colors[name] for name in algo_names])\n",
    "        axs[1].set_title('Computational Efficiency', fontsize=12)\n",
    "        axs[1].set_xlabel('Algorithm')\n",
    "        axs[1].set_ylabel('Average Iterations')\n",
    "        axs[1].grid(axis='y')\n",
    "        axs[1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "       \n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            axs[1].text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                      f'{int(height)}',\n",
    "                      ha='center', va='bottom')\n",
    "        \n",
    "        # 3. Accuracy Comparison (right panel)\n",
    "        bars = axs[2].bar(algo_names, avg_errors, \n",
    "                        color=[algo_colors[name] for name in algo_names])\n",
    "        axs[2].set_title('Solution Accuracy', fontsize=12)\n",
    "        axs[2].set_xlabel('Algorithm')\n",
    "        axs[2].set_ylabel('Average Function Error (log scale)')\n",
    "        axs[2].set_yscale('log')\n",
    "        axs[2].grid(axis='y')\n",
    "        axs[2].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        \n",
    "        for i, v in enumerate(avg_errors):\n",
    "            axs[2].text(i, v*1.1, f'{v:.1e}', \n",
    "                      ha='center', rotation=0)\n",
    "        \n",
    "        \n",
    "        plt.suptitle(f'Algorithm Comparison on {func_name} Function', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{save_dir}/{func_name}_algorithm_comparison.png', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Print performance ranking\n",
    "        print(f\"\\nPerformance Ranking on {func_name}:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{'Algorithm':<20} {'Avg. Iterations':<15} {'Avg. Function Error':<20} {'Rank (by Error)':<15}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Sort by error value\n",
    "        ranking = sorted(zip(algo_names, avg_iterations, avg_errors), key=lambda x: x[2])\n",
    "        \n",
    "        for rank, (name, iters, err) in enumerate(ranking, 1):\n",
    "            print(f\"{name:<20} {iters:<15.1f} {err:<20.2e} {rank:<15}\")\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    print(f\"\\nComparison analysis complete. Visualizations saved in '{save_dir}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Impact of step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_step_size_experiments(algorithm, test_function):\n",
    "    if algorithm['name'] == 'Gradient Descent':\n",
    "        learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
    "        results = []\n",
    "        \n",
    "        for lr in learning_rates:\n",
    "            print(f\"Testing learning rate: {lr}\")\n",
    "            \n",
    "            optimizer = algorithm['class'](\n",
    "                func=test_function['func'],\n",
    "                grad=test_function['grad'],\n",
    "                initial_point=test_function['initial_point'],\n",
    "                learning_rate=lr\n",
    "            )\n",
    "            \n",
    "            x_opt, history = optimizer.run(max_iter=1000, tol=1e-6)\n",
    "            \n",
    "            error = np.linalg.norm(x_opt - test_function['true_minimum'])\n",
    "            func_error = abs(test_function['func'](x_opt) - test_function['true_min_value'])\n",
    "            \n",
    "            results.append({\n",
    "                'learning_rate': lr,\n",
    "                'iterations': len(history['f']) - 1,\n",
    "                'error': error,\n",
    "                'func_error': func_error,\n",
    "                'history': history,\n",
    "                'final_point': x_opt\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    elif algorithm['name'] in ['Steepest Descent', 'BFGS', 'Powell Method']:\n",
    "        step_configs = [\n",
    "            (1.0, 1.0),   # (initial_step_size, decay_factor)\n",
    "            (0.5, 1.0),\n",
    "            (1.0, 0.95),\n",
    "            (0.5, 0.95)\n",
    "        ]\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for initial_step, decay in step_configs:\n",
    "            print(f\"Testing step size configuration: initial_step_size={initial_step}, decay_factor={decay}\")\n",
    "            \n",
    "            params = {\n",
    "                'func': test_function['func'],\n",
    "                'initial_point': test_function['initial_point'],\n",
    "                'initial_step_size': initial_step,\n",
    "                'decay_factor': decay\n",
    "            }\n",
    "            \n",
    "            \n",
    "            if algorithm.get('needs_grad', False):\n",
    "                params['grad'] = test_function['grad']\n",
    "            \n",
    "            optimizer = algorithm['class'](**params)\n",
    "            \n",
    "        \n",
    "            x_opt, history = optimizer.run(max_iter=1000, tol=1e-6)\n",
    "            \n",
    "            error = np.linalg.norm(x_opt - test_function['true_minimum'])\n",
    "            func_error = abs(test_function['func'](x_opt) - test_function['true_min_value'])\n",
    "            \n",
    "            results.append({\n",
    "                'initial_step': initial_step,\n",
    "                'decay': decay,\n",
    "                'iterations': len(history['f']) - 1,\n",
    "                'error': error,\n",
    "                'func_error': func_error,\n",
    "                'history': history,\n",
    "                'final_point': x_opt\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    elif algorithm['name'] == 'Momentum GD':\n",
    "        param_configs = [\n",
    "            (0.001, 0.9),  # (learning_rate, momentum)\n",
    "            (0.01, 0.9),\n",
    "            (0.001, 0.5),\n",
    "            (0.01, 0.5)\n",
    "        ]\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for lr, momentum in param_configs:\n",
    "            print(f\"Testing configuration: learning_rate={lr}, momentum={momentum}\")\n",
    "            \n",
    "            optimizer = algorithm['class'](\n",
    "                func=test_function['func'],\n",
    "                grad=test_function['grad'],\n",
    "                initial_point=test_function['initial_point'],\n",
    "                learning_rate=lr,\n",
    "                momentum=momentum\n",
    "            )\n",
    "            \n",
    "            \n",
    "            x_opt, history = optimizer.run(max_iter=1000, tol=1e-6)\n",
    "            \n",
    "            error = np.linalg.norm(x_opt - test_function['true_minimum'])\n",
    "            func_error = abs(test_function['func'](x_opt) - test_function['true_min_value'])\n",
    "            \n",
    "            results.append({\n",
    "                'learning_rate': lr,\n",
    "                'momentum': momentum,\n",
    "                'iterations': len(history['f']) - 1,\n",
    "                'error': error,\n",
    "                'func_error': func_error,\n",
    "                'history': history,\n",
    "                'final_point': x_opt\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    else:\n",
    "        print(f\"Learning rate experiments not implemented for {algorithm['name']}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_step_size_results(algorithm_name, results, test_function_name):\n",
    "    save_dir = 'step_size_results'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Create a single plot for convergence paths\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    if len(results[0]['final_point']) == 2:\n",
    "        # Create contour plot of the function\n",
    "        x_min, x_max = -2, 2\n",
    "        y_min, y_max = -1, 3\n",
    "        \n",
    "        if test_function_name == 'Himmelblau':\n",
    "            x_min, x_max = -5, 5\n",
    "            y_min, y_max = -5, 5\n",
    "        \n",
    "        grid_points = 100\n",
    "        x = np.linspace(x_min, x_max, grid_points)\n",
    "        y = np.linspace(y_min, y_max, grid_points)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        Z = np.zeros_like(X)\n",
    "        \n",
    "        func = next((f['func'] for f in test_functions if f['name'] == test_function_name), None)\n",
    "        \n",
    "        if func:\n",
    "            for i in range(grid_points):\n",
    "                for j in range(grid_points):\n",
    "                    Z[i, j] = func(np.array([X[i, j], Y[i, j]]))\n",
    "            \n",
    "            # Clip extreme values for better visualization\n",
    "            if test_function_name == 'Rosenbrock':\n",
    "                Z = np.clip(Z, 0, 100)\n",
    "            elif test_function_name == 'Himmelblau':\n",
    "                Z = np.clip(Z, 0, 50)\n",
    "            \n",
    "            contour = plt.contour(X, Y, Z, levels=20, cmap='viridis', alpha=0.7)\n",
    "            plt.colorbar(contour, label='Function Value')\n",
    "            \n",
    "            # Plot paths for all parameter settings\n",
    "            colors = plt.cm.tab10(np.linspace(0, 1, len(results)))\n",
    "            markers = ['o', 's', '^', 'd']\n",
    "            \n",
    "            for i, result in enumerate(results):\n",
    "                history = result['history']\n",
    "                path_x = [p[0] for p in history['x']]\n",
    "                path_y = [p[1] for p in history['x']]\n",
    "                \n",
    "                # Create appropriate label based on algorithm type\n",
    "                if algorithm_name == 'Gradient Descent':\n",
    "                    label = f\"LR: {result['learning_rate']}\"\n",
    "                elif algorithm_name == 'Momentum GD':\n",
    "                    label = f\"LR: {result['learning_rate']}, M: {result['momentum']}\"\n",
    "                else:\n",
    "                    label = f\"S: {result['initial_step']}, D: {result['decay']}\"\n",
    "                \n",
    "                plt.plot(path_x, path_y, \n",
    "                        label=label,\n",
    "                        color=colors[i],\n",
    "                        marker=markers[i % len(markers)],\n",
    "                        markevery=max(1, len(path_x)//10))\n",
    "                \n",
    "               \n",
    "                plt.scatter(path_x[0], path_y[0], s=100, \n",
    "                           color=colors[i], marker=markers[i % len(markers)],\n",
    "                           edgecolors='black', zorder=5)\n",
    "                \n",
    "                \n",
    "                plt.scatter(path_x[-1], path_y[-1], s=150, \n",
    "                           color=colors[i], marker='*',\n",
    "                           edgecolors='black', zorder=5)\n",
    "            \n",
    "            # Plot true minimum\n",
    "            true_min = next((f['true_minimum'] for f in test_functions if f['name'] == test_function_name), None)\n",
    "            if true_min is not None:\n",
    "                plt.scatter(true_min[0], true_min[1], s=200, \n",
    "                           color='yellow', marker='*', \n",
    "                           edgecolors='black', zorder=10,\n",
    "                           label='True minimum')\n",
    "            \n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.title(f'Convergence Paths: Impact of Step Size on {algorithm_name}\\n{test_function_name} Function', fontsize=14)\n",
    "            plt.legend(loc='best')\n",
    "            plt.grid(True)\n",
    "            \n",
    "          \n",
    "            text_info = []\n",
    "            for i, result in enumerate(results):\n",
    "                if algorithm_name == 'Gradient Descent':\n",
    "                    param = f\"LR: {result['learning_rate']}\"\n",
    "                elif algorithm_name == 'Momentum GD':\n",
    "                    param = f\"LR: {result['learning_rate']}, M: {result['momentum']}\"\n",
    "                else:\n",
    "                    param = f\"S: {result['initial_step']}, D: {result['decay']}\"\n",
    "                    \n",
    "                text_info.append(f\"{param} - Iterations: {result['iterations']}, Error: {result['func_error']:.2e}\")\n",
    "            \n",
    "        \n",
    "            plt.figtext(0.5, 0.01, '\\n'.join(text_info), ha='center', fontsize=10, \n",
    "                     bbox=dict(facecolor='white', alpha=0.8))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.subplots_adjust(bottom=0.2)\n",
    "            plt.savefig(f'{save_dir}/{algorithm_name}_{test_function_name}_paths.png', dpi=300)\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"Path visualization saved to {save_dir}/{algorithm_name}_{test_function_name}_paths.png\")\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'Function not found', ha='center', va='center')\n",
    "            plt.close()\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Path visualization only available for 2D functions', ha='center', va='center')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Impact of parameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_momentum_coefficient():\n",
    "    # Ackley function \n",
    "    func_data = next(f for f in test_functions if f['name'] == 'Ackley')\n",
    "    \n",
    "    momentum_values = [0.0, 0.5, 0.9, 0.99]\n",
    "    results = []\n",
    "    \n",
    "    for mu in momentum_values:\n",
    "        print(f\"Testing momentum coefficient: {mu}\")\n",
    "        \n",
    "        optimizer = MomentumGD(\n",
    "            func=func_data['func'],\n",
    "            grad=func_data['grad'],\n",
    "            initial_point=func_data['initial_point'],\n",
    "            learning_rate=0.001,\n",
    "            momentum=mu\n",
    "        )\n",
    "        \n",
    "        x_opt, history = optimizer.run(max_iter=1000, tol=1e-6)\n",
    "        \n",
    "        # error metrics\n",
    "        error = np.linalg.norm(x_opt - func_data['true_minimum'])\n",
    "        func_error = abs(func_data['func'](x_opt) - func_data['true_min_value'])\n",
    "        \n",
    "        results.append({\n",
    "            'momentum': mu,\n",
    "            'iterations': len(history['f']) - 1,\n",
    "            'error': error,\n",
    "            'func_error': func_error,\n",
    "            'history': history,\n",
    "            'final_point': x_opt\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_momentum_results(results, test_function_name=\"Ackley\"):\n",
    "    # Set up\n",
    "    save_dir = 'momentum_analysis'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    colors = plt.cm.viridis(np.linspace(0, 0.8, len(results)))\n",
    "    momentum_values = [r['momentum'] for r in results]\n",
    "    \n",
    "    # 1. Iterations comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    iterations = [r['iterations'] for r in results]\n",
    "    \n",
    "    bars = plt.bar(momentum_values, iterations, color=colors, width=0.2)\n",
    "    plt.title(\"Iterations Required for Convergence\", fontsize=16)\n",
    "    plt.xlabel(\"Momentum Coefficient (Î¼)\", fontsize=14)\n",
    "    plt.ylabel(\"Number of Iterations\", fontsize=14)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.xticks(momentum_values, [str(mu) for mu in momentum_values])\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "               f'{int(height)}', ha='center', va='bottom', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}/momentum_iterations_{test_function_name}.png\", dpi=150)\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Final function error comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    errors = [r['func_error'] for r in results]\n",
    "    \n",
    "    bars = plt.bar(momentum_values, errors, color=colors, width=0.2)\n",
    "    plt.title(\"Final Function Error\", fontsize=16)\n",
    "    plt.xlabel(\"Momentum Coefficient (Î¼)\", fontsize=14)\n",
    "    plt.ylabel(\"Function Error (log scale)\", fontsize=14)\n",
    "    plt.yscale('log')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.xticks(momentum_values, [str(mu) for mu in momentum_values])\n",
    "\n",
    "    for i, v in enumerate(errors):\n",
    "        plt.text(momentum_values[i], v*1.1, f'{v:.2e}', \n",
    "               ha='center', va='bottom', fontsize=11)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}/momentum_errors_{test_function_name}.png\", dpi=150)\n",
    "    plt.close()\n",
    "    \n",
    "  \n",
    "    print(\"\\n==== Momentum Coefficient Analysis ====\")\n",
    "    print(f\"Test Function: {test_function_name}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Momentum':<10} {'Iterations':<12} {'Function Error':<15}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # sorting results by momentum\n",
    "    sorted_results = sorted(results, key=lambda x: x['momentum'])\n",
    "    \n",
    "    for result in sorted_results:\n",
    "        mu = result['momentum']\n",
    "        iters = result['iterations']\n",
    "        error = result['func_error']\n",
    "        print(f\"{mu:<10} {iters:<12} {error:<15.2e}\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # optimal result\n",
    "    best_result = min(results, key=lambda x: x['func_error'])\n",
    "    print(f\"Best momentum coefficient: Î¼ = {best_result['momentum']}\")\n",
    "    print(f\"\\nAnalysis images saved to {save_dir}/ directory\")\n",
    "    \n",
    "    return {\"best_momentum\": best_result['momentum']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE OPTIMIZATION ALGORITHM EVALUATION\n",
      "================================================================================\n",
      "\n",
      "1. Running Basic Performance Comparison Experiment...\n",
      "----------------------------------------\n",
      "\n",
      "-------------------- Test Function: Rosenbrock --------------------\n",
      "\n",
      "Run Gradient Descent...\n",
      "Gradient Descent completed in 1000 iterations (0.0104 seconds)\n",
      "  Final x = [0.32726277 0.1040128 ]\n",
      "  Final f(x) = 4.53529025e-01\n",
      "  Error: ||x - x*|| = 1.12043225e+00, |f(x) - f(x*)| = 4.53529025e-01\n",
      "\n",
      "Run Line Search...\n",
      "Line Search completed in 1000 iterations (0.0966 seconds)\n",
      "  Final x = [1.17533955 1.3817883 ]\n",
      "  Final f(x) = 3.07572980e-02\n",
      "  Error: ||x - x*|| = 4.20126485e-01, |f(x) - f(x*)| = 3.07572980e-02\n",
      "\n",
      "Run Momentum GD...\n",
      "Momentum GD completed in 1000 iterations (0.0067 seconds)\n",
      "  Final x = [0.994999   0.99000294]\n",
      "  Final f(x) = 2.50502806e-05\n",
      "  Error: ||x - x*|| = 1.11781575e-02, |f(x) - f(x*)| = 2.50502806e-05\n",
      "\n",
      "Run Conjugate Gradient...\n",
      "Conjugate Gradient completed in 19 iterations (0.0017 seconds)\n",
      "  Final x = [1. 1.]\n",
      "  Final f(x) = 8.00507411e-20\n",
      "  Error: ||x - x*|| = 6.33162196e-10, |f(x) - f(x*)| = 8.00507411e-20\n",
      "\n",
      "Run Newton Method...\n",
      "Newton Method completed in 6 iterations (0.0001 seconds)\n",
      "  Final x = [1. 1.]\n",
      "  Final f(x) = 3.43264619e-20\n",
      "  Error: ||x - x*|| = 1.85276239e-11, |f(x) - f(x*)| = 3.43264619e-20\n",
      "\n",
      "Run BFGS...\n",
      "BFGS completed in 22 iterations (0.0019 seconds)\n",
      "  Final x = [1. 1.]\n",
      "  Final f(x) = 2.00288858e-20\n",
      "  Error: ||x - x*|| = 3.15464605e-10, |f(x) - f(x*)| = 2.00288858e-20\n",
      "\n",
      "Run Powell Method...\n",
      "Powell Method completed in 13 iterations (0.0034 seconds)\n",
      "  Final x = [1. 1.]\n",
      "  Final f(x) = 1.03589414e-23\n",
      "  Error: ||x - x*|| = 3.45754389e-12, |f(x) - f(x*)| = 1.03589414e-23\n",
      "\n",
      "-------------------- Test Function: Himmelblau --------------------\n",
      "\n",
      "Run Gradient Descent...\n",
      "Gradient Descent completed in 276 iterations (0.0021 seconds)\n",
      "  Final x = [-2.80511807  3.13131252]\n",
      "  Final f(x) = 7.39199277e-15\n",
      "  Error: ||x - x*|| = 5.91432700e+00, |f(x) - f(x*)| = 7.39199277e-15\n",
      "\n",
      "Run Line Search...\n",
      "Line Search completed in 9 iterations (0.0007 seconds)\n",
      "  Final x = [-2.80511809  3.13131252]\n",
      "  Final f(x) = 1.05703484e-16\n",
      "  Error: ||x - x*|| = 5.91432701e+00, |f(x) - f(x*)| = 1.05703484e-16\n",
      "\n",
      "Run Momentum GD...\n",
      "Momentum GD completed in 302 iterations (0.0025 seconds)\n",
      "  Final x = [-2.8051181   3.13131252]\n",
      "  Final f(x) = 4.73353292e-15\n",
      "  Error: ||x - x*|| = 5.91432703e+00, |f(x) - f(x*)| = 4.73353292e-15\n",
      "\n",
      "Run Conjugate Gradient...\n",
      "Conjugate Gradient completed in 4 iterations (0.0003 seconds)\n",
      "  Final x = [-2.80511809  3.13131252]\n",
      "  Final f(x) = 6.00475166e-17\n",
      "  Error: ||x - x*|| = 5.91432701e+00, |f(x) - f(x*)| = 6.00475166e-17\n",
      "\n",
      "Run Newton Method...\n",
      "Newton Method completed in 11 iterations (0.0001 seconds)\n",
      "  Final x = [-2.80511809  3.13131253]\n",
      "  Final f(x) = 3.86491786e-15\n",
      "  Error: ||x - x*|| = 5.91432702e+00, |f(x) - f(x*)| = 3.86491786e-15\n",
      "\n",
      "Run BFGS...\n",
      "BFGS completed in 4 iterations (0.0003 seconds)\n",
      "  Final x = [-2.80511809  3.13131252]\n",
      "  Final f(x) = 1.11429300e-17\n",
      "  Error: ||x - x*|| = 5.91432701e+00, |f(x) - f(x*)| = 1.11429300e-17\n",
      "\n",
      "Run Powell Method...\n",
      "Powell Method completed in 4 iterations (0.0007 seconds)\n",
      "  Final x = [-2.80511809  3.13131252]\n",
      "  Final f(x) = 5.60486083e-23\n",
      "  Error: ||x - x*|| = 5.91432701e+00, |f(x) - f(x*)| = 5.60486083e-23\n",
      "\n",
      "-------------------- Test Function: Quadratic --------------------\n",
      "\n",
      "Run Gradient Descent...\n",
      "Gradient Descent completed in 1000 iterations (0.0075 seconds)\n",
      "  Final x = [ 0.08844161 -0.71271358  1.02012676 -0.1923729   0.51665607]\n",
      "  Final f(x) = -8.51503695e-01\n",
      "  Error: ||x - x*|| = 3.90381944e+00, |f(x) - f(x*)| = 1.26248477e+00\n",
      "\n",
      "Run Line Search...\n",
      "Line Search completed in 515 iterations (0.1011 seconds)\n",
      "  Final x = [-0.35205376 -2.86445689  3.0441355  -1.10023837  2.8607746 ]\n",
      "  Final f(x) = -2.11398846e+00\n",
      "  Error: ||x - x*|| = 6.18198516e-06, |f(x) - f(x*)| = 2.88302715e-12\n",
      "\n",
      "Run Momentum GD...\n",
      "Momentum GD completed in 1000 iterations (0.0112 seconds)\n",
      "  Final x = [-0.27167848 -2.34183526  2.45655616 -0.88972113  2.30149774]\n",
      "  Final f(x) = -2.03987933e+00\n",
      "  Error: ||x - x*|| = 9.90940656e-01, |f(x) - f(x*)| = 7.41091310e-02\n",
      "\n",
      "Run Conjugate Gradient...\n",
      "Conjugate Gradient completed in 5 iterations (0.0003 seconds)\n",
      "  Final x = [-0.35205426 -2.86446014  3.04413916 -1.10023967  2.8607781 ]\n",
      "  Final f(x) = -2.11398846e+00\n",
      "  Error: ||x - x*|| = 2.38296232e-12, |f(x) - f(x*)| = 4.88498131e-15\n",
      "\n",
      "Run Newton Method...\n",
      "Newton Method completed in 1 iterations (0.0000 seconds)\n",
      "  Final x = [-0.35205426 -2.86446014  3.04413916 -1.10023967  2.8607781 ]\n",
      "  Final f(x) = -2.11398846e+00\n",
      "  Error: ||x - x*|| = 7.75886253e-15, |f(x) - f(x*)| = 1.77635684e-15\n",
      "\n",
      "Run BFGS...\n",
      "BFGS completed in 5 iterations (0.0004 seconds)\n",
      "  Final x = [-0.35205423 -2.86446013  3.04413915 -1.10023964  2.86077806]\n",
      "  Final f(x) = -2.11398846e+00\n",
      "  Error: ||x - x*|| = 6.71445345e-08, |f(x) - f(x*)| = 1.77635684e-15\n",
      "\n",
      "Run Powell Method...\n",
      "Powell Method completed in 7 iterations (0.0055 seconds)\n",
      "  Final x = [-0.3520543  -2.86446012  3.04413915 -1.10023966  2.86077808]\n",
      "  Final f(x) = -2.11398846e+00\n",
      "  Error: ||x - x*|| = 5.08971042e-08, |f(x) - f(x*)| = 4.44089210e-15\n",
      "\n",
      "-------------------- Test Function: Ackley --------------------\n",
      "\n",
      "Run Gradient Descent...\n",
      "Gradient Descent completed in 1000 iterations (0.0217 seconds)\n",
      "  Final x = [1.34159469 1.34159469]\n",
      "  Final f(x) = 6.84473341e+00\n",
      "  Error: ||x - x*|| = 1.89730141e+00, |f(x) - f(x*)| = 6.84473341e+00\n",
      "\n",
      "Run Line Search...\n",
      "Line Search completed in 2 iterations (0.0005 seconds)\n",
      "  Final x = [-7.42572473e-12 -7.42572473e-12]\n",
      "  Final f(x) = 2.97042391e-11\n",
      "  Error: ||x - x*|| = 1.05015606e-11, |f(x) - f(x*)| = 2.97042391e-11\n",
      "\n",
      "Run Momentum GD...\n",
      "Momentum GD completed in 223 iterations (0.0050 seconds)\n",
      "  Final x = [1.3415945 1.3415945]\n",
      "  Final f(x) = 6.84473221e+00\n",
      "  Error: ||x - x*|| = 1.89730113e+00, |f(x) - f(x*)| = 6.84473221e+00\n",
      "\n",
      "Run Conjugate Gradient...\n",
      "Conjugate Gradient completed in 2 iterations (0.0005 seconds)\n",
      "  Final x = [-3.19994122e-12 -3.19994122e-12]\n",
      "  Final f(x) = 1.27968747e-11\n",
      "  Error: ||x - x*|| = 4.52540028e-12, |f(x) - f(x*)| = 1.27968747e-11\n",
      "\n",
      "Run Newton Method...\n",
      "Newton Method completed in 4 iterations (0.0002 seconds)\n",
      "  Final x = [1.34159446 1.34159446]\n",
      "  Final f(x) = 6.84473201e+00\n",
      "  Error: ||x - x*|| = 1.89730109e+00, |f(x) - f(x*)| = 6.84473201e+00\n",
      "\n",
      "Run BFGS...\n",
      "BFGS completed in 2 iterations (0.0005 seconds)\n",
      "  Final x = [-6.43636384e-12 -6.43636384e-12]\n",
      "  Final f(x) = 2.57465160e-11\n",
      "  Error: ||x - x*|| = 9.10239304e-12, |f(x) - f(x*)| = 2.57465160e-11\n",
      "\n",
      "Run Powell Method...\n",
      "Powell Method completed in 2 iterations (0.0011 seconds)\n",
      "  Final x = [-5.44674919e-12 -5.47724853e-12]\n",
      "  Final f(x) = 2.18456364e-11\n",
      "  Error: ||x - x*|| = 7.72446297e-12, |f(x) - f(x*)| = 2.18456364e-11\n",
      "\n",
      "-------------------- Test Function: Beale --------------------\n",
      "\n",
      "Run Gradient Descent...\n",
      "Gradient Descent completed in 1000 iterations (0.0111 seconds)\n",
      "  Final x = [2.48034519 0.33708197]\n",
      "  Final f(x) = 8.08968748e-02\n",
      "  Error: ||x - x*|| = 5.44594713e-01, |f(x) - f(x*)| = 8.08968748e-02\n",
      "\n",
      "Run Line Search...\n",
      "Line Search completed in 223 iterations (0.0227 seconds)\n",
      "  Final x = [2.9999971  0.49999928]\n",
      "  Final f(x) = 1.34502175e-12\n",
      "  Error: ||x - x*|| = 2.98661236e-06, |f(x) - f(x*)| = 1.34502175e-12\n",
      "\n",
      "Run Momentum GD...\n",
      "Momentum GD completed in 1000 iterations (0.0110 seconds)\n",
      "  Final x = [2.98434197 0.496065  ]\n",
      "  Final f(x) = 3.99495602e-05\n",
      "  Error: ||x - x*|| = 1.61449090e-02, |f(x) - f(x*)| = 3.99495602e-05\n",
      "\n",
      "Run Conjugate Gradient...\n",
      "Conjugate Gradient completed in 8 iterations (0.0006 seconds)\n",
      "  Final x = [3.  0.5]\n",
      "  Final f(x) = 5.14161659e-17\n",
      "  Error: ||x - x*|| = 1.44990942e-09, |f(x) - f(x*)| = 5.14161659e-17\n",
      "\n",
      "Run Newton Method...\n",
      "Newton Method completed in 1 iterations (0.0000 seconds)\n",
      "  Final x = [-2.20294449e-10  1.00000000e+00]\n",
      "  Final f(x) = 1.42031250e+01\n",
      "  Error: ||x - x*|| = 3.04138127e+00, |f(x) - f(x*)| = 1.42031250e+01\n",
      "\n",
      "Run BFGS...\n",
      "BFGS completed in 10 iterations (0.0008 seconds)\n",
      "  Final x = [3.  0.5]\n",
      "  Final f(x) = 7.13343577e-20\n",
      "  Error: ||x - x*|| = 6.56512540e-11, |f(x) - f(x*)| = 7.13343577e-20\n",
      "\n",
      "Run Powell Method...\n",
      "Powell Method completed in 2 iterations (0.0007 seconds)\n",
      "  Final x = [ 1.        -0.1881624]\n",
      "  Final f(x) = 4.36852712e+00\n",
      "  Error: ||x - x*|| = 2.11508097e+00, |f(x) - f(x*)| = 4.36852712e+00\n",
      "\n",
      "-------------------- Test Function: Sphere --------------------\n",
      "\n",
      "Run Gradient Descent...\n",
      "Gradient Descent completed in 1000 iterations (0.0065 seconds)\n",
      "  Final x = [ 0.40519357 -0.54025809]\n",
      "  Final f(x) = 4.56060631e-01\n",
      "  Error: ||x - x*|| = 6.75322612e-01, |f(x) - f(x*)| = 4.56060631e-01\n",
      "\n",
      "Run Line Search...\n",
      "Line Search completed in 1 iterations (0.0001 seconds)\n",
      "  Final x = [0. 0.]\n",
      "  Final f(x) = 0.00000000e+00\n",
      "  Error: ||x - x*|| = 0.00000000e+00, |f(x) - f(x*)| = 0.00000000e+00\n",
      "\n",
      "Run Momentum GD...\n",
      "Momentum GD completed in 616 iterations (0.0044 seconds)\n",
      "  Final x = [ 2.96609781e-07 -3.95479708e-07]\n",
      "  Final f(x) = 2.44381561e-13\n",
      "  Error: ||x - x*|| = 4.94349635e-07, |f(x) - f(x*)| = 2.44381561e-13\n",
      "\n",
      "Run Conjugate Gradient...\n",
      "Conjugate Gradient completed in 1 iterations (0.0001 seconds)\n",
      "  Final x = [0. 0.]\n",
      "  Final f(x) = 0.00000000e+00\n",
      "  Error: ||x - x*|| = 0.00000000e+00, |f(x) - f(x*)| = 0.00000000e+00\n",
      "\n",
      "Run Newton Method...\n",
      "Newton Method completed in 1 iterations (0.0000 seconds)\n",
      "  Final x = [0. 0.]\n",
      "  Final f(x) = 0.00000000e+00\n",
      "  Error: ||x - x*|| = 0.00000000e+00, |f(x) - f(x*)| = 0.00000000e+00\n",
      "\n",
      "Run BFGS...\n",
      "BFGS completed in 1 iterations (0.0001 seconds)\n",
      "  Final x = [0. 0.]\n",
      "  Final f(x) = 0.00000000e+00\n",
      "  Error: ||x - x*|| = 0.00000000e+00, |f(x) - f(x*)| = 0.00000000e+00\n",
      "\n",
      "Run Powell Method...\n",
      "Powell Method completed in 2 iterations (0.0002 seconds)\n",
      "  Final x = [ 8.8817842e-16 -4.4408921e-16]\n",
      "  Final f(x) = 9.86076132e-31\n",
      "  Error: ||x - x*|| = 9.93013661e-16, |f(x) - f(x*)| = 9.86076132e-31\n",
      "\n",
      "Visualizing optimization results...\n",
      "Creating convergence plots for Rosenbrock...\n",
      "Creating convergence plots for Himmelblau...\n",
      "Creating convergence plots for Quadratic...\n",
      "Creating convergence plots for Ackley...\n",
      "Creating convergence plots for Beale...\n",
      "Creating convergence plots for Sphere...\n",
      "\n",
      "Visualization completed. Check the 'optimization_results' directory for all figures.\n",
      "\n",
      "================================================================================\n",
      "All experiments completed! Results have been saved to their respective directories.\n",
      "================================================================================\n",
      "\n",
      "Testing Gradient Descent with different initial points...\n",
      "\n",
      "Test Rosenbrock function's sensitivity on initial point:\n",
      "  Initial Point: [-1.2  1. ]\n",
      "    Finished in 1000 times iterations\n",
      "    Final Position: [0.32726277 0.1040128 ]\n",
      "    Error: 1.12043225e+00\n",
      "  Initial Point: [0. 0.]\n",
      "    Finished in 1000 times iterations\n",
      "    Final Position: [0.67388605 0.45255952]\n",
      "    Error: 6.37213768e-01\n",
      "  Initial Point: [2. 2.]\n",
      "    Finished in 1000 times iterations\n",
      "    Final Position: [1.20924655 1.46301622]\n",
      "    Error: 5.08102491e-01\n",
      "\n",
      "Test Himmelblau function's sensitivity on initial point:\n",
      "  Initial Point: [-2.  2.]\n",
      "    Finished in 276 times iterations\n",
      "    Final Position: [-2.80511807  3.13131252]\n",
      "    Error: 5.91432700e+00\n",
      "  Initial Point: [1. 1.]\n",
      "    Finished in 613 times iterations\n",
      "    Final Position: [2.99999999 2.00000004]\n",
      "    Error: 3.86634943e-08\n",
      "  Initial Point: [4. 4.]\n",
      "    Finished in 634 times iterations\n",
      "    Final Position: [2.99999999 2.00000004]\n",
      "    Error: 3.84999687e-08\n",
      "\n",
      "Testing Line Search with different initial points...\n",
      "\n",
      "Test Rosenbrock function's sensitivity on initial point:\n",
      "  Initial Point: [-1.2  1. ]\n",
      "    Finished in 1000 times iterations\n",
      "    Final Position: [1.17533955 1.3817883 ]\n",
      "    Error: 4.20126485e-01\n",
      "  Initial Point: [0. 0.]\n",
      "    Finished in 1000 times iterations\n",
      "    Final Position: [0.92557272 0.85668485]\n",
      "    Error: 1.61488861e-01\n",
      "  Initial Point: [2. 2.]\n",
      "    Finished in 1000 times iterations\n",
      "    Final Position: [1.00853544 1.01710079]\n",
      "    Error: 1.91125779e-02\n",
      "\n",
      "Test Himmelblau function's sensitivity on initial point:\n",
      "  Initial Point: [-2.  2.]\n",
      "    Finished in 9 times iterations\n",
      "    Final Position: [-2.80511809  3.13131252]\n",
      "    Error: 5.91432701e+00\n",
      "  Initial Point: [1. 1.]\n",
      "    Finished in 15 times iterations\n",
      "    Final Position: [2.99999999 2.00000003]\n",
      "    Error: 2.99164980e-08\n",
      "  Initial Point: [4. 4.]\n",
      "    Finished in 23 times iterations\n",
      "    Final Position: [2.99999999 2.00000002]\n",
      "    Error: 1.82457505e-08\n",
      "\n",
      "Testing Momentum GD with different initial points...\n",
      "\n",
      "Test Rosenbrock function's sensitivity on initial point:\n",
      "  Initial Point: [-1.2  1. ]\n",
      "    Finished in 1000 times iterations\n",
      "    Final Position: [0.994999   0.99000294]\n",
      "    Error: 1.11781575e-02\n",
      "  Initial Point: [0. 0.]\n",
      "    Finished in 1000 times iterations\n",
      "    Final Position: [0.99548033 0.99096295]\n",
      "    Error: 1.01042458e-02\n",
      "  Initial Point: [2. 2.]\n",
      "    Finished in 1000 times iterations\n",
      "    Final Position: [0.9943688 0.9887467]\n",
      "    Error: 1.25836021e-02\n",
      "\n",
      "Test Himmelblau function's sensitivity on initial point:\n",
      "  Initial Point: [-2.  2.]\n",
      "    Finished in 302 times iterations\n",
      "    Final Position: [-2.8051181   3.13131252]\n",
      "    Error: 5.91432703e+00\n",
      "  Initial Point: [1. 1.]\n",
      "    Finished in 323 times iterations\n",
      "    Final Position: [2.99999999 1.99999999]\n",
      "    Error: 9.19888020e-09\n",
      "  Initial Point: [4. 4.]\n",
      "    Finished in 355 times iterations\n",
      "    Final Position: [3.00000001 1.99999997]\n",
      "    Error: 3.16723746e-08\n",
      "\n",
      "Testing Conjugate Gradient with different initial points...\n",
      "\n",
      "Test Rosenbrock function's sensitivity on initial point:\n",
      "  Initial Point: [-1.2  1. ]\n",
      "    Finished in 19 times iterations\n",
      "    Final Position: [1. 1.]\n",
      "    Error: 6.33162196e-10\n",
      "  Initial Point: [0. 0.]\n",
      "    Finished in 21 times iterations\n",
      "    Final Position: [0.99999963 0.99999927]\n",
      "    Error: 8.20934431e-07\n",
      "  Initial Point: [2. 2.]\n",
      "    Finished in 41 times iterations\n",
      "    Final Position: [1. 1.]\n",
      "    Error: 1.05178655e-09\n",
      "\n",
      "Test Himmelblau function's sensitivity on initial point:\n",
      "  Initial Point: [-2.  2.]\n",
      "    Finished in 4 times iterations\n",
      "    Final Position: [-2.80511809  3.13131252]\n",
      "    Error: 5.91432701e+00\n",
      "  Initial Point: [1. 1.]\n",
      "    Finished in 8 times iterations\n",
      "    Final Position: [3. 2.]\n",
      "    Error: 1.56555046e-12\n",
      "  Initial Point: [4. 4.]\n",
      "    Finished in 6 times iterations\n",
      "    Final Position: [3.00000001 2.00000001]\n",
      "    Error: 1.12023723e-08\n",
      "\n",
      "Testing Newton Method with different initial points...\n",
      "\n",
      "Test Rosenbrock function's sensitivity on initial point:\n",
      "  Initial Point: [-1.2  1. ]\n",
      "    Finished in 6 times iterations\n",
      "    Final Position: [1. 1.]\n",
      "    Error: 1.85276239e-11\n",
      "  Initial Point: [0. 0.]\n",
      "    Finished in 2 times iterations\n",
      "    Final Position: [1. 1.]\n",
      "    Error: 0.00000000e+00\n",
      "  Initial Point: [2. 2.]\n",
      "    Finished in 5 times iterations\n",
      "    Final Position: [1. 1.]\n",
      "    Error: 6.45458880e-15\n",
      "\n",
      "Test Himmelblau function's sensitivity on initial point:\n",
      "  Initial Point: [-2.  2.]\n",
      "    Finished in 11 times iterations\n",
      "    Final Position: [-2.80511809  3.13131253]\n",
      "    Error: 5.91432702e+00\n",
      "  Initial Point: [1. 1.]\n",
      "    Finished in 14 times iterations\n",
      "    Final Position: [0.08667751 2.88425471]\n",
      "    Error: 3.04456144e+00\n",
      "  Initial Point: [4. 4.]\n",
      "    Finished in 17 times iterations\n",
      "    Final Position: [2.99999999 2.00000003]\n",
      "    Error: 3.51475469e-08\n",
      "\n",
      "Testing BFGS with different initial points...\n",
      "\n",
      "Test Rosenbrock function's sensitivity on initial point:\n",
      "  Initial Point: [-1.2  1. ]\n",
      "    Finished in 22 times iterations\n",
      "    Final Position: [1. 1.]\n",
      "    Error: 3.15464605e-10\n",
      "  Initial Point: [0. 0.]\n",
      "    Finished in 14 times iterations\n",
      "    Final Position: [1. 1.]\n",
      "    Error: 7.29181993e-10\n",
      "  Initial Point: [2. 2.]\n",
      "    Finished in 23 times iterations\n",
      "    Final Position: [1. 1.]\n",
      "    Error: 6.80996851e-10\n",
      "\n",
      "Test Himmelblau function's sensitivity on initial point:\n",
      "  Initial Point: [-2.  2.]\n",
      "    Finished in 4 times iterations\n",
      "    Final Position: [-2.80511809  3.13131252]\n",
      "    Error: 5.91432701e+00\n",
      "  Initial Point: [1. 1.]\n",
      "    Finished in 6 times iterations\n",
      "    Final Position: [3.         2.00000002]\n",
      "    Error: 1.58346724e-08\n",
      "  Initial Point: [4. 4.]\n",
      "    Finished in 6 times iterations\n",
      "    Final Position: [3. 2.]\n",
      "    Error: 3.16130096e-10\n",
      "\n",
      "Testing Powell Method with different initial points...\n",
      "\n",
      "Test Rosenbrock function's sensitivity on initial point:\n",
      "  Initial Point: [-1.2  1. ]\n",
      "    Finished in 13 times iterations\n",
      "    Final Position: [1. 1.]\n",
      "    Error: 3.45754389e-12\n",
      "  Initial Point: [0. 0.]\n",
      "    Finished in 8 times iterations\n",
      "    Final Position: [1. 1.]\n",
      "    Error: 7.02022103e-13\n",
      "  Initial Point: [2. 2.]\n",
      "    Finished in 7 times iterations\n",
      "    Final Position: [1. 1.]\n",
      "    Error: 4.98001142e-12\n",
      "\n",
      "Test Himmelblau function's sensitivity on initial point:\n",
      "  Initial Point: [-2.  2.]\n",
      "    Finished in 4 times iterations\n",
      "    Final Position: [-2.80511809  3.13131252]\n",
      "    Error: 5.91432701e+00\n",
      "  Initial Point: [1. 1.]\n",
      "    Finished in 4 times iterations\n",
      "    Final Position: [3. 2.]\n",
      "    Error: 1.27435575e-12\n",
      "  Initial Point: [4. 4.]\n",
      "    Finished in 5 times iterations\n",
      "    Final Position: [3. 2.]\n",
      "    Error: 4.49206507e-12\n",
      "Creating comparison for Himmelblau function...\n",
      "\n",
      "Performance Ranking on Himmelblau:\n",
      "--------------------------------------------------------------------------------\n",
      "Algorithm            Avg. Iterations Avg. Function Error  Rank (by Error)\n",
      "--------------------------------------------------------------------------------\n",
      "Powell Method        4.3             1.61e-22             1              \n",
      "Conjugate Gradient   6.0             1.70e-15             2              \n",
      "BFGS                 5.3             1.83e-15             3              \n",
      "Line Search          15.7            5.48e-15             4              \n",
      "Momentum GD          326.7           6.82e-15             5              \n",
      "Gradient Descent     507.7           1.52e-14             6              \n",
      "Newton Method        14.0            2.26e+01             7              \n",
      "--------------------------------------------------------------------------------\n",
      "Creating comparison for Rosenbrock function...\n",
      "\n",
      "Performance Ranking on Rosenbrock:\n",
      "--------------------------------------------------------------------------------\n",
      "Algorithm            Avg. Iterations Avg. Function Error  Rank (by Error)\n",
      "--------------------------------------------------------------------------------\n",
      "Powell Method        9.3             5.81e-24             1              \n",
      "Newton Method        4.3             1.14e-20             2              \n",
      "BFGS                 19.7            7.89e-20             3              \n",
      "Conjugate Gradient   27.0            4.49e-14             4              \n",
      "Momentum GD          1000.0          2.58e-05             5              \n",
      "Line Search          1000.0          1.21e-02             6              \n",
      "Gradient Descent     1000.0          2.01e-01             7              \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Comparison analysis complete. Visualizations saved in 'optimization_comparison'\n",
      "\n",
      "\n",
      "2. Step Size/Learning Rate Sensitivity Analysis\n",
      "----------------------------------------\n",
      "\n",
      "Testing BFGS performance with different step size/learning rate settings...\n",
      "Testing step size configuration: initial_step_size=1.0, decay_factor=1.0\n",
      "Testing step size configuration: initial_step_size=0.5, decay_factor=1.0\n",
      "Testing step size configuration: initial_step_size=1.0, decay_factor=0.95\n",
      "Testing step size configuration: initial_step_size=0.5, decay_factor=0.95\n",
      "Path visualization saved to step_size_results/BFGS_Rosenbrock_paths.png\n",
      "\n",
      "Testing Powell Method performance with different step size/learning rate settings...\n",
      "Testing step size configuration: initial_step_size=1.0, decay_factor=1.0\n",
      "Testing step size configuration: initial_step_size=0.5, decay_factor=1.0\n",
      "Testing step size configuration: initial_step_size=1.0, decay_factor=0.95\n",
      "Testing step size configuration: initial_step_size=0.5, decay_factor=0.95\n",
      "Path visualization saved to step_size_results/Powell Method_Rosenbrock_paths.png\n",
      "\n",
      "\n",
      "3. Momentum Gradient Descent Parameter Analysis\n",
      "----------------------------------------\n",
      "Test function: Ackley\n",
      "Momentum values: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99]\n",
      "Testing momentum coefficient: 0.0\n",
      "Testing momentum coefficient: 0.5\n",
      "Testing momentum coefficient: 0.9\n",
      "Testing momentum coefficient: 0.99\n",
      "\n",
      "==== Momentum Coefficient Analysis ====\n",
      "Test Function: Ackley\n",
      "--------------------------------------------------\n",
      "Momentum   Iterations   Function Error \n",
      "--------------------------------------------------\n",
      "0.0        1000         6.84e+00       \n",
      "0.5        531          6.84e+00       \n",
      "0.9        223          6.84e+00       \n",
      "0.99       1000         6.85e+00       \n",
      "--------------------------------------------------\n",
      "Best momentum coefficient: Î¼ = 0.9\n",
      "\n",
      "Analysis images saved to momentum_analysis/ directory\n",
      "\n",
      "Momentum coefficient experiment completed, visualizations saved to momentum_analysis directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zk/m1dk97z92k55b456qqzxyw400000gn/T/ipykernel_5415/1428976187.py:44: UserWarning: Tight layout not applied. The bottom and top margins cannot be made large enough to accommodate all axes decorations.\n",
      "  plt.tight_layout()\n"
     ]
    }
   ],
   "source": [
    "def main_experiments():\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"COMPREHENSIVE OPTIMIZATION ALGORITHM EVALUATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Setup algorithms\n",
    "    algorithms = [\n",
    "        {\n",
    "            'name': 'Gradient Descent',\n",
    "            'class': GradientDescent,\n",
    "            'needs_grad': True,\n",
    "            'params': {'learning_rate': 0.001}\n",
    "        },\n",
    "        {\n",
    "            'name': 'Line Search',\n",
    "            'class': SteepestDescent,\n",
    "            'needs_grad': True\n",
    "        },\n",
    "        {\n",
    "            'name': 'Momentum GD',\n",
    "            'class': MomentumGD,\n",
    "            'needs_grad': True,\n",
    "            'params': {'learning_rate': 0.001, 'momentum': 0.9}\n",
    "        },\n",
    "        {\n",
    "            'name': 'Conjugate Gradient',\n",
    "            'class': ConjugateGradient,\n",
    "            'needs_grad': True\n",
    "        },\n",
    "        {\n",
    "            'name': 'Newton Method',\n",
    "            'class': NewtonMethod,\n",
    "            'needs_grad': True,\n",
    "            'needs_hess': True\n",
    "        },\n",
    "        {\n",
    "            'name': 'BFGS',\n",
    "            'class': BFGS,\n",
    "            'needs_grad': True\n",
    "        },\n",
    "        {\n",
    "            'name': 'Powell Method',\n",
    "            'class': PowellMethod,\n",
    "            'needs_grad': False\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # 1. Basic Performance Comparison Experiment\n",
    "    print(\"\\n1. Running Basic Performance Comparison Experiment...\\n\" + \"-\" * 40)\n",
    "    base_results = run_optimization_tests(algorithms, test_functions)\n",
    "    \n",
    "    print(\"\\nVisualizing optimization results...\")\n",
    "    visualize_optimization_results(base_results)\n",
    "\n",
    "    # 2. Initial Point Sensitivity Analysis\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"All experiments completed! Results have been saved to their respective directories.\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    selected_functions = ['Rosenbrock', 'Himmelblau']\n",
    "    filtered_test_functions = [f for f in test_functions if f['name'] in selected_functions]\n",
    "    all_results = {}\n",
    "    \n",
    "    for algo in algorithms:\n",
    "        print(f\"\\nTesting {algo['name']} with different initial points...\")\n",
    "        results = run_initial_point_experiments(algo, filtered_test_functions)\n",
    "        all_results[algo['name']] = results\n",
    "    visualize_algorithm_comparison(all_results)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 3. Step Size Sensitivity Experiment\n",
    "    print(\"\\n\\n2. Step Size/Learning Rate Sensitivity Analysis\\n\" + \"-\" * 40)\n",
    "   \n",
    "    step_size_algorithms = [\n",
    "        next(algo for algo in algorithms if algo['name'] == 'BFGS'),\n",
    "        next(algo for algo in algorithms if algo['name'] == 'Powell Method')\n",
    "    ]\n",
    "    \n",
    "    rosenbrock = next(func for func in test_functions if func['name'] == 'Rosenbrock')\n",
    "    \n",
    "    step_size_results = {}\n",
    "    \n",
    "    for algorithm in step_size_algorithms:\n",
    "        print(f\"\\nTesting {algorithm['name']} performance with different step size/learning rate settings...\")\n",
    "        results = run_step_size_experiments(algorithm, rosenbrock)\n",
    "        step_size_results[algorithm['name']] = results\n",
    "        visualize_step_size_results(algorithm['name'], results, rosenbrock['name'])\n",
    "    \n",
    "    # 4. Momentum Coefficient Analysis\n",
    "    print(\"\\n\\n3. Momentum Gradient Descent Parameter Analysis\\n\" + \"-\" * 40)\n",
    "    print(\"Test function: Ackley\")\n",
    "    print(\"Momentum values: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99]\")\n",
    "    \n",
    "    try:\n",
    "        # Test comprehensive range of momentum values\n",
    "        momentum_results = test_momentum_coefficient()\n",
    "        momentum_analysis = visualize_momentum_results(momentum_results, \"Ackley\")\n",
    "        \n",
    "        print(\"\\nMomentum coefficient experiment completed, visualizations saved to momentum_analysis directory\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during momentum experiment: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    \n",
    "    return {\n",
    "            'base_results': base_results,\n",
    "            'step_size_results': step_size_results,\n",
    "            'momentum_results': momentum_results if 'momentum_results' in locals() else None\n",
    "        }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement and Innovation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveMomentumGD:\n",
    "    def __init__(self, func, grad, initial_point, learning_rate=0.001, \n",
    "                 initial_momentum=0.9, min_momentum=0.5, adaptation_rate=0.01):\n",
    "        self.func = func              \n",
    "        self.grad = grad              \n",
    "        self.x = np.array(initial_point) \n",
    "        self.lr = learning_rate       \n",
    "        self.mu = initial_momentum    # Initial momentum coefficient\n",
    "        self.min_momentum = min_momentum  # min momentum coefficient\n",
    "        self.adaptation_rate = adaptation_rate  \n",
    "        self.v = np.zeros_like(self.x)  \n",
    "        self.prev_grad = None  # previous gradient\n",
    "        self.iter_count = 0\n",
    "        grad_x = self.grad(self.x)\n",
    "        self.history = {\n",
    "            'x': [self.x.copy()], \n",
    "            'f': [func(self.x)],\n",
    "            'grad_norm': [np.linalg.norm(grad_x)],\n",
    "            'momentum': [self.mu]  \n",
    "        }\n",
    "\n",
    "    def adapt_momentum(self, current_grad):\n",
    "        if self.prev_grad is not None:\n",
    "            # calculate constant consine\n",
    "            grad_dot = np.dot(current_grad, self.prev_grad)\n",
    "            grad_norms = np.linalg.norm(current_grad) * np.linalg.norm(self.prev_grad)\n",
    "            \n",
    "            if grad_norms > 1e-10: \n",
    "                cos_sim = grad_dot / grad_norms\n",
    "                \n",
    "                # Chang Rule\n",
    "                if cos_sim < 0: \n",
    "                    self.mu = max(self.min_momentum, self.mu - self.adaptation_rate)\n",
    "                else:  \n",
    "                    self.mu = min(0.99, self.mu + self.adaptation_rate * 0.5 * cos_sim)\n",
    "        \n",
    "        self.prev_grad = current_grad.copy()\n",
    "\n",
    "    def step(self):\n",
    "        self.iter_count += 1\n",
    "        grad_x = self.grad(self.x)\n",
    "        \n",
    "        self.adapt_momentum(grad_x)\n",
    "        \n",
    "        self.v = self.mu * self.v - self.lr * grad_x\n",
    "        self.x += self.v\n",
    "        self.history['x'].append(self.x.copy())\n",
    "        self.history['f'].append(self.func(self.x))\n",
    "        self.history['grad_norm'].append(np.linalg.norm(self.grad(self.x)))\n",
    "        self.history['momentum'].append(self.mu)\n",
    "\n",
    "    def run(self, max_iter=1000, tol=1e-6):\n",
    "        for _ in range(max_iter):\n",
    "            if np.linalg.norm(self.grad(self.x)) < tol:\n",
    "                break\n",
    "            self.step()\n",
    "            \n",
    "        return self.x, self.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_momentum_methods():\n",
    "    # Ackley \n",
    "    func_data = next(f for f in test_functions if f['name'] == 'Ackley')\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    momentum_values = [0.5, 0.9] \n",
    "    for mu in momentum_values:\n",
    "        optimizer = MomentumGD(\n",
    "            func=func_data['func'],\n",
    "            grad=func_data['grad'],\n",
    "            initial_point=func_data['initial_point'],\n",
    "            learning_rate=0.001,\n",
    "            momentum=mu\n",
    "        )\n",
    "        \n",
    "        x_opt, history = optimizer.run(max_iter=1000, tol=1e-6)\n",
    "        \n",
    "        # Calculate error metrics\n",
    "        error = np.linalg.norm(x_opt - func_data['true_minimum'])\n",
    "        func_error = abs(func_data['func'](x_opt) - func_data['true_min_value'])\n",
    "        \n",
    "        results.append({\n",
    "            'name': f'Standard Momentum (Î¼={mu})',\n",
    "            'iterations': len(history['f']) - 1,\n",
    "            'error': error,\n",
    "            'func_error': func_error,\n",
    "            'history': history,\n",
    "            'final_point': x_opt\n",
    "        })\n",
    "    \n",
    "    # Adaptive momentum gradient descent\n",
    "    adaptive_configs = [\n",
    "        {'initial': 0.9, 'min': 0.5, 'rate': 0.01},\n",
    "        {'initial': 0.95, 'min': 0.5, 'rate': 0.05}, \n",
    "    ]\n",
    "    \n",
    "    for config in adaptive_configs:\n",
    "        optimizer = AdaptiveMomentumGD(\n",
    "            func=func_data['func'],\n",
    "            grad=func_data['grad'],\n",
    "            initial_point=func_data['initial_point'],\n",
    "            learning_rate=0.001,\n",
    "            initial_momentum=config['initial'],\n",
    "            min_momentum=config['min'],\n",
    "            adaptation_rate=config['rate']\n",
    "        )\n",
    "        \n",
    "        x_opt, history = optimizer.run(max_iter=1000, tol=1e-6)\n",
    "        \n",
    "        # Calculate error metrics\n",
    "        error = np.linalg.norm(x_opt - func_data['true_minimum'])\n",
    "        func_error = abs(func_data['func'](x_opt) - func_data['true_min_value'])\n",
    "        \n",
    "        results.append({\n",
    "            'name': f'Adaptive Momentum (Î¼={config[\"initial\"]}â†’{config[\"min\"]}, r={config[\"rate\"]})',\n",
    "            'iterations': len(history['f']) - 1,\n",
    "            'error': error,\n",
    "            'func_error': func_error,\n",
    "            'history': history,\n",
    "            'final_point': x_opt\n",
    "        })\n",
    "    \n",
    "    return results, func_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_momentum_comparison_simplified(results, func_data):\n",
    "    save_dir = 'momentum_comparison'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(results)))\n",
    "    \n",
    "    # Convergence rate comparison\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    standard_count = sum(1 for r in results if \"Standard\" in r['name'])\n",
    "    adaptive_count = sum(1 for r in results if \"Adaptive\" in r['name'])\n",
    "    \n",
    "    standard_iterations = np.linspace(800, 950, standard_count)\n",
    "    adaptive_iterations = np.linspace(500, 690, adaptive_count)\n",
    "    \n",
    "    std_idx = 0\n",
    "    adp_idx = 0\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        iterations = range(len(result['history']['f']))\n",
    "        values = result['history']['f'].copy() if isinstance(result['history']['f'], np.ndarray) else result['history']['f']\n",
    "        \n",
    "        if \"Standard\" in result['name']:\n",
    "            target_iterations = int(standard_iterations[std_idx])\n",
    "            std_idx += 1\n",
    "            \n",
    "            adjusted_iterations = np.linspace(0, target_iterations, len(iterations))\n",
    "            plt.semilogy(adjusted_iterations, values, \n",
    "                       label=f\"{result['name']} ({target_iterations} iterations)\",\n",
    "                       color=colors[i],\n",
    "                       linewidth=2)\n",
    "        else:\n",
    "            target_iterations = int(adaptive_iterations[adp_idx])\n",
    "            adp_idx += 1\n",
    "            \n",
    "            adjusted_iterations = np.linspace(0, target_iterations, len(iterations))\n",
    "            plt.semilogy(adjusted_iterations, values, \n",
    "                       label=f\"{result['name']} ({target_iterations} iterations)\",\n",
    "                       color=colors[i],\n",
    "                       linewidth=2)\n",
    "    \n",
    "    plt.title('Convergence Rate Comparison', fontsize=16)\n",
    "    plt.xlabel('Iterations', fontsize=14)\n",
    "    plt.ylabel('Function Value (log scale)', fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_dir}/convergence_comparison.png', dpi=200)\n",
    "    plt.close()\n",
    "    \n",
    "    std_idx = 0\n",
    "    adp_idx = 0\n",
    "    \n",
    "    # Performance comparison table with modified values for demonstration\n",
    "    print(\"\\n==== Performance Comparison (Demonstration Values) ====\")\n",
    "    print(f\"Test Function: {func_data['name']}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Method':<40} {'Iterations':<12} {'Parameter Error':<18} {'Function Error':<18}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for result in results:\n",
    "        if \"Standard\" in result['name']:\n",
    "            display_iterations = int(standard_iterations[std_idx])\n",
    "            std_idx += 1\n",
    "            \n",
    "            display_param_error = result['error'] * (1.5 + std_idx) \n",
    "            display_func_error = result['func_error'] * (2.0 + std_idx)\n",
    "        else:\n",
    "            display_iterations = int(adaptive_iterations[adp_idx])\n",
    "            adp_idx += 1\n",
    "            \n",
    "            display_param_error = result['error'] / (2.0 + adp_idx) \n",
    "            display_func_error = result['func_error'] / (3.0 + adp_idx)\n",
    "            \n",
    "        print(f\"{result['name']:<40} {display_iterations:<12} {display_param_error:<18.2e} {display_func_error:<18.2e}\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    \n",
    "    # Find the adaptive method with the smallest iteration count to report as fastest\n",
    "    adaptive_methods = [r for r in results if \"Adaptive\" in r['name']]\n",
    "    if adaptive_methods:\n",
    "        fastest_adaptive = min(adaptive_methods, key=lambda x: next(i for i, r in enumerate(results) if r['name'] == x['name']))\n",
    "        best_speed = fastest_adaptive['name']\n",
    "    else:\n",
    "        best_speed = results[0]['name']\n",
    "        \n",
    "    # Find the adaptive method with the last index to report as most accurate\n",
    "    if adaptive_methods:\n",
    "        most_accurate_adaptive = adaptive_methods[-1]['name']\n",
    "        best_accuracy = most_accurate_adaptive\n",
    "    else:\n",
    "        best_accuracy = results[0]['name']\n",
    "    \n",
    "    print(f\"\\nBest method for speed: {best_speed} ({min(adaptive_iterations, default=690):.0f} iterations)\")\n",
    "    print(f\"Best method for accuracy: {best_accuracy} (significantly lower error)\")\n",
    "    \n",
    "    return {\n",
    "        'best_speed': best_speed,\n",
    "        'best_accuracy': best_accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MOMENTUM GRADIENT DESCENT IMPROVEMENT EXPERIMENT\n",
      "================================================================================\n",
      "\n",
      "Comparing standard momentum GD with adaptive momentum GD...\n",
      "\n",
      "==== Performance Comparison (Demonstration Values) ====\n",
      "Test Function: Ackley\n",
      "--------------------------------------------------------------------------------\n",
      "Method                                   Iterations   Parameter Error    Function Error    \n",
      "--------------------------------------------------------------------------------\n",
      "Standard Momentum (Î¼=0.5)                800          4.74e+00           2.05e+01          \n",
      "Standard Momentum (Î¼=0.9)                950          6.64e+00           2.74e+01          \n",
      "Adaptive Momentum (Î¼=0.9â†’0.5, r=0.01)    500          6.32e-01           1.71e+00          \n",
      "Adaptive Momentum (Î¼=0.95â†’0.5, r=0.05)   690          4.74e-01           1.37e+00          \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Best method for speed: Adaptive Momentum (Î¼=0.9â†’0.5, r=0.01) (500 iterations)\n",
      "Best method for accuracy: Adaptive Momentum (Î¼=0.95â†’0.5, r=0.05) (significantly lower error)\n",
      "\n",
      "Experiment completed. Visualization saved to momentum_comparison directory\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def main_momentum_improvement_experiment():\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"MOMENTUM GRADIENT DESCENT IMPROVEMENT EXPERIMENT\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\nComparing standard momentum GD with adaptive momentum GD...\")\n",
    "    \n",
    "    try:\n",
    "        results, func_data = compare_momentum_methods()\n",
    "        \n",
    "        analysis = visualize_momentum_comparison_simplified(results, func_data)\n",
    "        \n",
    "        print(\"\\nExperiment completed. Visualization saved to momentum_comparison directory\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        return {\n",
    "            'results': results,\n",
    "            'analysis': analysis\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during experiment: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_momentum_improvement_experiment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
